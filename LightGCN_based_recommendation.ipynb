{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9665b6e474ed4cfe8449c4bbfeee60ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c0f5258984947e1a8d8b658329ed8d0",
              "IPY_MODEL_4ad7115a014747b4965453fa04ab4e9c",
              "IPY_MODEL_93cf28df996748318ecb8ff7f632aaa9"
            ],
            "layout": "IPY_MODEL_894c1e55f0d147d589d297f71a53b390"
          }
        },
        "5c0f5258984947e1a8d8b658329ed8d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d0c9ccf8ed74682beb5288477cfd9d6",
            "placeholder": "​",
            "style": "IPY_MODEL_95187c08eb0345129e0f776deec6640f",
            "value": "100%"
          }
        },
        "4ad7115a014747b4965453fa04ab4e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_312663e29f3d46acb0c61efa2eda56bb",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d02678654384f6480f69a820c6e411c",
            "value": 10000
          }
        },
        "93cf28df996748318ecb8ff7f632aaa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dee1e0251b34407b7ed9b5e33f507b8",
            "placeholder": "​",
            "style": "IPY_MODEL_732df694dee24a89b3013cdf5086abbf",
            "value": " 10000/10000 [12:53&lt;00:00, 18.52it/s]"
          }
        },
        "894c1e55f0d147d589d297f71a53b390": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d0c9ccf8ed74682beb5288477cfd9d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95187c08eb0345129e0f776deec6640f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "312663e29f3d46acb0c61efa2eda56bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d02678654384f6480f69a820c6e411c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1dee1e0251b34407b7ed9b5e33f507b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "732df694dee24a89b3013cdf5086abbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUAdzQTOX9vO",
        "outputId": "e4c4857c-9b32-4bab-cdff-ed3ec6fc1961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Loading the dataset\n",
        "def loaddata(filename):\n",
        "    df = pd.read_csv(f'/content/drive/MyDrive/data-brm/{filename}.csv', sep=',', encoding='latin-1')\n",
        "    return df\n",
        "\n",
        "def loaddata2():\n",
        "    df = pd.read_csv(f'/content/drive/MyDrive/cleaned_book_data.csv', sep=',', encoding='latin-1')\n",
        "    return df\n",
        "\n",
        "books   = loaddata2()\n",
        "ratings = loaddata(\"ratings\")"
      ],
      "metadata": {
        "id": "aTDSPYAnYPPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVQwtWu6dPvj",
        "outputId": "2a1435f9-53a1-4fe6-8a4c-0e63153e2fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TsQPBzbZwNU",
        "outputId": "1b94ab61-d912-452b-a927-26cb67943d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1+pt20cu118\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.17%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.23.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.17+pt20cu118\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.23.5)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.1+pt20cu118\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-hwfezv9e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-hwfezv9e\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 9e687b412e87aa0f07ce7152e31d6f40271ed86d\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric==2.4.0) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric==2.4.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric==2.4.0) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric==2.4.0) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric==2.4.0) (3.2.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.4.0-py3-none-any.whl size=992429 sha256=2c088b36a18ccd5f659fd92b4ab9df2711f14f78d603597c465956c739bfbb18\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0p42hhka/wheels/d3/78/eb/9e26525b948d19533f1688fb6c209cec8a0ba793d39b49ae8f\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required modules\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection, metrics, preprocessing\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, Tensor\n",
        "\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from torch_geometric.utils import structured_negative_sampling\n",
        "from torch_geometric.data import download_url, extract_zip\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.typing import Adj\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "r4-NJ1MlZcxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset already loaded. now preprocessing"
      ],
      "metadata": {
        "id": "yE-_EYuTbXHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rating_users = ratings['user_id'].value_counts().reset_index().\\\n",
        "               rename({'index':'user_id','user_id':'rating'}, axis=1)"
      ],
      "metadata": {
        "id": "-adDAfZIcbSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rating_books = ratings['book_id'].value_counts().reset_index().\\\n",
        "               rename({'index':'book_id','book_id':'rating'}, axis=1)"
      ],
      "metadata": {
        "id": "s5KR7xjBcb9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = ratings[ratings['user_id'].isin(rating_users[rating_users['rating']>5]['user_id'])]\n",
        "ratings = ratings[ratings['book_id'].isin(rating_books[rating_books['rating']> 2000]['book_id'])]"
      ],
      "metadata": {
        "id": "avH4RPnTcduj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = ratings.head(650000)\n",
        "ratings = temp"
      ],
      "metadata": {
        "id": "t4Nv49S5cff8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrY0RBWzl-Ry",
        "outputId": "ed832e27-a43d-4a0f-e059-bd33c4ba907c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(650000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using LabelEncoder to convert categorical user/book id to numerical ids\n",
        "lbl_user = preprocessing.LabelEncoder()\n",
        "lbl_books = preprocessing.LabelEncoder()\n",
        "\n",
        "# converting user/book id into numerical ids\n",
        "ratings.user_id = lbl_user.fit_transform(ratings.user_id.values)\n",
        "ratings.book_id = lbl_books.fit_transform(ratings.book_id.values)"
      ],
      "metadata": {
        "id": "dlBA2FJ6bWRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lbl_user)\n",
        "print(lbl_books)\n",
        "print(ratings.user_id)\n",
        "print(ratings.book_id)"
      ],
      "metadata": {
        "id": "oII2WIuSlu3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.head(20)"
      ],
      "metadata": {
        "id": "pDHciBjgcu56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ratings.user_id.max())\n",
        "print(ratings.book_id.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG6L9y69brPN",
        "outputId": "0eb712dd-609e-4338-ecea-fb63afb38b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16270\n",
            "505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from the dataframe and convert it into a form suitable for graph-recommender system\n",
        "# src_index_col, dst_index_col, link_index_col represents the column names of user id, book id and rating\n",
        "def load_edge_csv(df,\n",
        "                  src_index_col,\n",
        "                  dst_index_col,\n",
        "                  link_index_col,\n",
        "                  rating_threshold=3.5):\n",
        "    \"\"\"Loads csv containing edges between users and items\n",
        "\n",
        "    Args:\n",
        "        path (str): path to csv file\n",
        "        src_index_col (str): column name of users\n",
        "        src_mapping (dict): mapping between row number and user id\n",
        "        dst_index_col (str): column name of items\n",
        "        dst_mapping (dict): mapping between row number and item id\n",
        "        link_index_col (str): column name of user item interaction\n",
        "        rating_threshold (int, optional): Threshold to determine positivity of edge. Defaults to 4.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 2 by N matrix containing the node ids of N user-item edges\n",
        "    \"\"\"\n",
        "\n",
        "    edge_index = None     #this will hold edge indices\n",
        "    src = [user_id for user_id in  df['user_id']]   #stores user id's as list\n",
        "\n",
        "    num_users = len(df['user_id'].unique())         #number of  unique user id's\n",
        "\n",
        "    dst = [(book_id) for book_id in df['book_id']]   #stores book id's as list\n",
        "\n",
        "    link_vals = df[link_index_col].values     #stores the ratings between user and books as list\n",
        "\n",
        "\n",
        "    # binary attribute(0/1) indicating whether the rating is greater than or equal to a rating_threshold.\n",
        "    #helps determine whether edge is positive or not\n",
        "    edge_attr = torch.from_numpy(df[link_index_col].values).view(-1, 1).to(torch.long) >= rating_threshold\n",
        "\n",
        "    edge_values = []\n",
        "\n",
        "    # The code creates the edge_index list of lists, where edge_index[0]\n",
        "    # contains the user IDs (source nodes) of the edges, and edge_index[1]\n",
        "    # contains the item IDs (destination nodes) of the edges.\n",
        "    # These edges are determined based on the edge_attr values.\n",
        "    edge_index = [[], []]\n",
        "\n",
        "\n",
        "    #add values to edge_index and edge_values.\n",
        "    # edge_index -> 0: user_id and 1:book_id\n",
        "    # edge_values -> ratings\n",
        "    # only enters the loop if edge_attr element is a positive entry\n",
        "    for i in range(edge_attr.shape[0]):\n",
        "        if edge_attr[i]:\n",
        "            edge_index[0].append(src[i])\n",
        "            edge_index[1].append(dst[i])\n",
        "            edge_values.append(link_vals[i])\n",
        "\n",
        "    # edge_values is the label we will use for compute training loss\n",
        "    return edge_index, edge_values"
      ],
      "metadata": {
        "id": "gnUUz0fUbyNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "edge_index, edge_values = load_edge_csv(\n",
        "    ratings,\n",
        "    src_index_col='user_id',\n",
        "    dst_index_col='book_id',\n",
        "    link_index_col='rating',\n",
        "    rating_threshold=1)\n",
        "\n",
        "# print(\"0: \",edge_index[0][:20])\n",
        "# print(\"1: \",edge_index[1])\n",
        "# print(\"values: \",edge_values)\n"
      ],
      "metadata": {
        "id": "68dMzNmgcCFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to tensor\n",
        "# We use LongTensor here because the .propagate() method in the model needs either LongTensor or SparseTensor\n",
        "print(\"0: \",edge_index[0][:20])\n",
        "print(\"1: \",edge_index[1][:20])\n",
        "print(\"values: \",edge_values[:20])\n",
        "\n",
        "edge_index = torch.LongTensor(edge_index)\n",
        "edge_values = torch.tensor(edge_values)\n",
        "\n",
        "print(edge_index)\n",
        "print(edge_index.size())\n",
        "\n",
        "print(edge_values)\n",
        "print(edge_values.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfI4pQMbcOZv",
        "outputId": "acecf664-c5f1-4f0e-8c55-932f07f6109b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:  [0, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "1:  [244, 25, 32, 278, 68, 248, 347, 17, 26, 20, 1, 22, 23, 101, 241, 34, 265, 307, 25, 82]\n",
            "values:  [5, 4, 4, 5, 4, 3, 4, 5, 5, 5, 5, 5, 5, 5, 2, 5, 3, 4, 3, 4]\n",
            "tensor([[    0,     1,     1,  ..., 11286, 15204, 15299],\n",
            "        [  244,    25,    32,  ...,   178,    42,    16]])\n",
            "torch.Size([2, 650000])\n",
            "tensor([5, 4, 4,  ..., 3, 5, 4])\n",
            "torch.Size([650000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-5BDGjvYOG8",
        "outputId": "87d6ec97-3e94-4f1a-ff20-bf8b8c9fdae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(650000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# num_users, num_books = len(user_mapping), len(book_mapping)\n",
        "\n",
        "num_users = len(ratings['user_id'].unique())\n",
        "num_books = len(ratings['book_id'].unique())\n",
        "\n",
        "print(f\"num_users {num_users}, num_books {num_books}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44G4O3Hfc_uv",
        "outputId": "14de1015-a9dc-4d01-bab7-181604d7a8c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_users 16271, num_books 506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to construct an adjacency matrix\n",
        "def convert_r_mat_edge_index_to_adj_mat_edge_index(input_edge_index, input_edge_values):\n",
        "\n",
        "    # R is initialised with 0's of size num_users x num_books\n",
        "    R = torch.zeros((num_users, num_books))\n",
        "    for i in range(len(input_edge_index[0])):\n",
        "        row_idx = input_edge_index[0][i]      #source\n",
        "        col_idx = input_edge_index[1][i]      #destination\n",
        "        R[row_idx][col_idx] = input_edge_values[i] # assign actual edge value to Interaction Matrix\n",
        "\n",
        "    # construct R transpose\n",
        "    R_transpose = torch.transpose(R, 0, 1)\n",
        "\n",
        "    # create adj_matrix\n",
        "    adj_mat = torch.zeros((num_users + num_books , num_users + num_books))\n",
        "\n",
        "    # upper left quedrant is filled with R\n",
        "    adj_mat[: num_users, num_users :] = R.clone()\n",
        "\n",
        "    # bottom right quadrant if filled with R_transpose\n",
        "    adj_mat[num_users :, : num_users] = R_transpose.clone()\n",
        "\n",
        "    # converts the adjacency matrix to sparse matrix\n",
        "    # this saves memory space by storing only non-zero values\n",
        "    adj_mat_coo = adj_mat.to_sparse_coo()\n",
        "\n",
        "    # stores indices of non-zero elements\n",
        "    adj_mat_coo_indices = adj_mat_coo.indices()\n",
        "\n",
        "    # stores the non-zero rating values\n",
        "    adj_mat_coo_values = adj_mat_coo.values()\n",
        "\n",
        "    # returns both\n",
        "    return adj_mat_coo_indices, adj_mat_coo_values"
      ],
      "metadata": {
        "id": "sm0MS5VRdoNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert adjacency matrix to original matrix form\n",
        "def convert_adj_mat_edge_index_to_r_mat_edge_index(input_edge_index, input_edge_values):\n",
        "\n",
        "  # Ensure input_edge_index and input_edge_values are on the same device as the model\n",
        "    input_edge_index = input_edge_index.to(device)\n",
        "    input_edge_values = input_edge_values.to(device)\n",
        "\n",
        "   # Create a sparse tensor from input_edge_index and input_edge_values\n",
        "    sparse_input_edge_index = torch.sparse_coo_tensor(input_edge_index, input_edge_values)\n",
        "\n",
        "    # Convert the sparse tensor to a dense tensor (adjacency matrix)\n",
        "    adj_mat = sparse_input_edge_index.to_dense()\n",
        "\n",
        "    # Extract the interaction matrix from the adjacency matrix\n",
        "    interact_mat = adj_mat[:num_users, num_users:]\n",
        "\n",
        "    # Convert the interaction matrix to a sparse tensor\n",
        "    r_mat_edge_index = interact_mat.to_sparse().indices()\n",
        "    r_mat_edge_values = interact_mat.to_sparse().values()\n",
        "\n",
        "    return r_mat_edge_index, r_mat_edge_values"
      ],
      "metadata": {
        "id": "vEGagu3AdulC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nF6LdVgAl-3",
        "outputId": "b620659c-b5f1-4f0d-e3c7-f0714cfc5550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 650000])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_interactions = edge_index.shape[1]\n",
        "all_indices = [i for i in range(num_interactions)]\n",
        "\n",
        "# splitting the datas for training, testing and validation by finding its indices. This finds the index range(columns) from the 2 x n edge_index list\n",
        "train_indices, test_indices = train_test_split(all_indices,\n",
        "                                               test_size=0.2,\n",
        "                                               random_state=1)\n",
        "\n",
        "val_indices, test_indices = train_test_split(test_indices,\n",
        "                                             test_size=0.5,\n",
        "                                             random_state=1)"
      ],
      "metadata": {
        "id": "tmUtH5gSdzAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_edge_index = edge_index[:, train_indices]\n",
        "train_edge_value = edge_values[train_indices]\n",
        "\n",
        "val_edge_index = edge_index[:, val_indices]\n",
        "val_edge_value = edge_values[val_indices]\n",
        "\n",
        "test_edge_index = edge_index[:, test_indices]\n",
        "test_edge_value = edge_values[test_indices]"
      ],
      "metadata": {
        "id": "CwhpfXpGd2oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_edge_index)\n",
        "print(train_edge_value)\n",
        "print(val_edge_index)\n",
        "print(val_edge_value)\n",
        "print(test_edge_index)\n",
        "print(test_edge_value)\n"
      ],
      "metadata": {
        "id": "HkcW_Ie_BbrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"num_users {num_users}, num_books {num_books}, num_interactions {num_interactions}\")\n",
        "print(f\"train_edge_index {train_edge_index}\")\n",
        "print((num_users + num_books))\n",
        "print(torch.unique(train_edge_index[0]).size())\n",
        "print(torch.unique(train_edge_index[1]).size())\n",
        "\n",
        "print(test_edge_value)\n",
        "print(test_edge_value.size())\n",
        "print(train_edge_value.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY4b-Aqkd447",
        "outputId": "1ac76977-66d7-43a2-e638-1fb2ef8d4e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_users 15059, num_books 505, num_interactions 600000\n",
            "train_edge_index tensor([[ 3791,  1595, 14385,  ..., 12618,  8281,  5520],\n",
            "        [   66,   265,    49,  ...,   195,   131,    92]])\n",
            "15564\n",
            "torch.Size([15003])\n",
            "torch.Size([505])\n",
            "tensor([5, 4, 5,  ..., 5, 5, 3])\n",
            "torch.Size([60000])\n",
            "torch.Size([480000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# converting regular matrix into adjacency matrix\n",
        "\n",
        "train_edge_index, train_edge_values  = convert_r_mat_edge_index_to_adj_mat_edge_index(train_edge_index, train_edge_value)\n",
        "val_edge_index, val_edge_values = convert_r_mat_edge_index_to_adj_mat_edge_index(val_edge_index, val_edge_value)\n",
        "test_edge_index, test_edge_values = convert_r_mat_edge_index_to_adj_mat_edge_index(test_edge_index, test_edge_value)"
      ],
      "metadata": {
        "id": "U5psJYRwd_Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_edge_index)\n",
        "print(train_edge_index.size())\n",
        "print(val_edge_index)\n",
        "print(val_edge_index.size())\n",
        "print(test_edge_index)\n",
        "print(test_edge_index.size())\n",
        "\n",
        "print(f\"\\n train_edge_values: \\n {train_edge_values} \\n {train_edge_values.size()}\")\n",
        "print(f\"\\n val_edge_values: \\n {val_edge_values} \\n {val_edge_values.size()}\")\n",
        "print(f\"\\n test_edge_values: \\n {test_edge_values} \\n {test_edge_values.size()}\")"
      ],
      "metadata": {
        "id": "y-aij613eJcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing LigthGCN"
      ],
      "metadata": {
        "id": "DVAGecfbeMsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defines LightGCN model\n",
        "class LightGCN(MessagePassing):\n",
        "    \"\"\"LightGCN Model as proposed in https://arxiv.org/abs/2002.02126\n",
        "    \"\"\"\n",
        "\n",
        "    # embedding_dim : each user/item will be represented in a 32 dimensional vector. The embedding vectors capture the\n",
        "                    # latent characteristics or features of users and items\n",
        "    def __init__(self, num_users, num_items, embedding_dim=32, K=3, add_self_loops=False, dropout_rate=0.1):\n",
        "        \"\"\"Initializes LightGCN Model\n",
        "\n",
        "        Args:\n",
        "            num_users (int): Number of users\n",
        "            num_items (int): Number of items\n",
        "            embedding_dim (int, optional): Dimensionality of embeddings. Defaults to 8.\n",
        "            K (int, optional): Number of message passing layers. Defaults to 3.\n",
        "            add_self_loops (bool, optional): Whether to add self loops for message passing. Defaults to False.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.K = K\n",
        "        self.add_self_loops = add_self_loops\n",
        "\n",
        "\n",
        "        # define user and item embedding for direct look up.\n",
        "        # embedding dimension: num_user/num_item x embedding_dim\n",
        "        self.users_emb = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.embedding_dim) # e_u^0\n",
        "\n",
        "        self.items_emb = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.embedding_dim) # e_i^0\n",
        "\n",
        "\n",
        "        # \"Fills the input Tensor with values drawn from the normal distribution\"\n",
        "        # according to LightGCN paper, this gives better performance\n",
        "        nn.init.normal_(self.users_emb.weight, std=0.1)\n",
        "        nn.init.normal_(self.items_emb.weight, std=0.1)\n",
        "\n",
        "        # create a linear layer (fully connected layer) so we can output a single value (predicted_rating)\n",
        "        self.out = nn.Linear(embedding_dim + embedding_dim, 1)\n",
        "\n",
        "    def forward(self, edge_index: Tensor, edge_values: Tensor):\n",
        "        \"\"\"Forward propagation of LightGCN Model.\n",
        "\n",
        "        Args:\n",
        "            edge_index (SparseTensor): adjacency matrix\n",
        "\n",
        "        Returns:\n",
        "            tuple (Tensor): e_u_k, e_u_0, e_i_k, e_i_0\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "            compute \\tilde{A}: symmetrically normalized adjacency matrix\n",
        "            \\tilde_A = D^(-1/2) * A * D^(-1/2)    according to LightGCN paper\n",
        "\n",
        "            this is essentially a metrix operation way to get 1/ (sqrt(n_neighbors_i) * sqrt(n_neighbors_j))\n",
        "\n",
        "\n",
        "            if your original edge_index look like\n",
        "            tensor([[   0,    0,    0,  ...,  609,  609,  609],\n",
        "                    [   0,    2,    5,  ..., 9444, 9445, 9485]])\n",
        "\n",
        "                    torch.Size([2, 99466])\n",
        "\n",
        "            then this will output:\n",
        "                (\n",
        "                 tensor([[   0,    0,    0,  ...,  609,  609,  609],\n",
        "                         [   0,    2,    5,  ..., 9444, 9445, 9485]]),\n",
        "                 tensor([0.0047, 0.0096, 0.0068,  ..., 0.0592, 0.0459, 0.1325])\n",
        "                 )\n",
        "\n",
        "              where edge_index_norm[0] is just the original edge_index\n",
        "\n",
        "              and edge_index_norm[1] is the symmetrically normalization term.\n",
        "\n",
        "            under the hood it's basically doing\n",
        "                def compute_gcn_norm(edge_index, emb):\n",
        "                    emb = emb.weight\n",
        "                    from_, to_ = edge_index\n",
        "                    deg = degree(to_, emb.size(0), dtype=emb.dtype)\n",
        "                    deg_inv_sqrt = deg.pow(-0.5)\n",
        "                    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "                    norm = deg_inv_sqrt[from_] * deg_inv_sqrt[to_]\n",
        "\n",
        "                    return norm\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        edge_index_norm = gcn_norm(edge_index=edge_index,\n",
        "                                   add_self_loops=self.add_self_loops)\n",
        "\n",
        "        # concat the user_emb and item_emb as the layer0 embing matrix\n",
        "        # size will be (n_users + n_items) x emb_vector_len.   e.g: 10334 x 64\n",
        "        emb_0 = torch.cat([self.users_emb.weight, self.items_emb.weight]) # E^0\n",
        "\n",
        "        embs = [emb_0] # save the layer0 emb to the embs list\n",
        "\n",
        "        # emb_k is the emb that we are actually going to push it through the graph layers\n",
        "        # as described in lightGCN paper formula 7\n",
        "        emb_k = emb_0\n",
        "\n",
        "        # push the embedding of all users and items through the Graph Model K times.\n",
        "        # K here is the number of layers\n",
        "        for i in range(self.K):\n",
        "            emb_k = self.propagate(edge_index=edge_index_norm[0], x=emb_k, norm=edge_index_norm[1])\n",
        "            embs.append(emb_k)\n",
        "\n",
        "\n",
        "        # this is doing the formula8 in LightGCN paper\n",
        "\n",
        "        # the stacked embs is a list of embedding matrix at each layer\n",
        "        #    it's of shape n_nodes x (n_layers + 1) x emb_vector_len.\n",
        "        #        e.g: torch.Size([10334, 4, 64])\n",
        "        embs = torch.stack(embs, dim=1)\n",
        "\n",
        "        # From LightGCn paper: \"In our experiments, we find that setting α_k uniformly as 1/(K + 1)\n",
        "        #    leads to good performance in general.\"\n",
        "        emb_final = torch.mean(embs, dim=1) # E^K\n",
        "\n",
        "        users_emb_final, items_emb_final = torch.split(emb_final,\n",
        "                                                       [self.num_users, self.num_items]) # splits into e_u^K and e_i^K\n",
        "\n",
        "\n",
        "        r_mat_edge_index, _ = convert_adj_mat_edge_index_to_r_mat_edge_index(edge_index, edge_values)\n",
        "\n",
        "        src, dest =  r_mat_edge_index[0], r_mat_edge_index[1]\n",
        "\n",
        "        # applying embedding lookup to get embeddings for src nodes and dest nodes in the edge list\n",
        "        user_embeds = users_emb_final[src]\n",
        "        item_embeds = items_emb_final[dest]\n",
        "\n",
        "        # output dim: edge_index_len x 128 (given 64 is the original emb_vector_len)\n",
        "        output = torch.cat([user_embeds, item_embeds], dim=1)\n",
        "\n",
        "        # push it through the linear layer\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        return norm.view(-1, 1) * x_j\n",
        "\n",
        "layers = 5\n",
        "model = LightGCN(num_users=num_users,\n",
        "                 num_items=num_books,\n",
        "                 K=layers)"
      ],
      "metadata": {
        "id": "CG0nLSbseMQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dont run here. this part is implemented in the next notebook(Graph based titled...)\n",
        "from google.colab import files\n",
        "\n",
        "# Use files.upload() to upload a file from your local system\n",
        "uploaded = files.upload()\n",
        "\n",
        "map_location=torch.device('cpu')\n",
        "# Load the uploaded model file\n",
        "model_path = 'trained_graph_rec_model.pth'  # Specify the filename\n",
        "model = torch.load(model_path,map_location)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "9BGW_0T_C_ZH",
        "outputId": "e9871ce2-5bff-4c4b-993a-e3c24b5d15c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-83cf85c1-de6a-4463-99a6-7f4720ae3dd1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-83cf85c1-de6a-4463-99a6-7f4720ae3dd1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving trained_graph_rec_model.pth to trained_graph_rec_model (1).pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "4AhwK-zleZWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define contants\n",
        "ITERATIONS = 10000\n",
        "EPOCHS = 10\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "LR = 1e-3\n",
        "ITERS_PER_EVAL = 200\n",
        "ITERS_PER_LR_DECAY = 200\n",
        "K = 10\n",
        "LAMBDA = 1e-6\n"
      ],
      "metadata": {
        "id": "6zZ1HTdVeahM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"BATCH_SIZE {BATCH_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sodFB2YHec-l",
        "outputId": "448c69ab-654c-412b-8a88-d4a4442272b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH_SIZE 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device {device}.\")\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "\n",
        "# add decay to avoid overfit\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=1.02)\n",
        "\n",
        "edge_index = edge_index.to(device)\n",
        "train_edge_index = train_edge_index.to(device)\n",
        "val_edge_index = val_edge_index.to(device)\n",
        "\n",
        "\n",
        "loss_func = nn.MSELoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-44k-K_efBf",
        "outputId": "4b7715b3-2cd2-43d7-ddaf-edee7e6cd675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recall_at_k(input_edge_index,\n",
        "                     input_edge_values, # the true label of actual ratings for each user/item interaction\n",
        "                     pred_ratings, # the list of predicted ratings\n",
        "                     k=10,\n",
        "                     threshold=3.5):\n",
        "    with torch.no_grad():\n",
        "        user_item_rating_list = defaultdict(list)\n",
        "\n",
        "        for i in range(len(input_edge_index[0])):\n",
        "            src = input_edge_index[0][i].item()\n",
        "            dest = input_edge_index[1][i].item()\n",
        "            true_rating = input_edge_values[i].item()\n",
        "            pred_rating = pred_ratings[i].item()\n",
        "\n",
        "            user_item_rating_list[src].append((pred_rating, true_rating))\n",
        "\n",
        "        recalls = dict()\n",
        "        precisions = dict()\n",
        "\n",
        "        for user_id, user_ratings in user_item_rating_list.items():\n",
        "            user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "            n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
        "\n",
        "            n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
        "\n",
        "            n_rel_and_rec_k = sum(\n",
        "                ((true_r >= threshold) and (est >= threshold))\n",
        "                for (est, true_r) in user_ratings[:k]\n",
        "            )\n",
        "\n",
        "            precisions[user_id] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
        "            recalls[user_id] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
        "\n",
        "        overall_recall = sum(rec for rec in recalls.values()) / len(recalls)\n",
        "        overall_precision = sum(prec for prec in precisions.values()) / len(precisions)\n",
        "\n",
        "        return overall_recall, overall_precision\n"
      ],
      "metadata": {
        "id": "S3PxwJmeekhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_edge_index"
      ],
      "metadata": {
        "id": "YIlntG5oE_tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r_mat_train_edge_index, r_mat_train_edge_values = convert_adj_mat_edge_index_to_r_mat_edge_index(train_edge_index, train_edge_values)\n",
        "r_mat_val_edge_index, r_mat_val_edge_values = convert_adj_mat_edge_index_to_r_mat_edge_index(val_edge_index, val_edge_values)\n",
        "r_mat_test_edge_index, r_mat_test_edge_values = convert_adj_mat_edge_index_to_r_mat_edge_index(test_edge_index, test_edge_values)"
      ],
      "metadata": {
        "id": "X3Xlee8gexrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_recall_at_ks = []\n",
        "\n",
        "for iter in tqdm(range(ITERATIONS)):\n",
        "    # forward propagation\n",
        "\n",
        "    # the rating here is based on r_mat\n",
        "    pred_ratings = model.forward(train_edge_index, train_edge_values)\n",
        "\n",
        "\n",
        "    train_loss = loss_func(pred_ratings, r_mat_train_edge_values.view(-1,1))\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # going over validation set\n",
        "    if iter % ITERS_PER_EVAL == 0:\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_pred_ratings = model.forward(val_edge_index, val_edge_values)\n",
        "\n",
        "            val_loss = loss_func(val_pred_ratings, r_mat_val_edge_values.view(-1,1)).sum()\n",
        "\n",
        "            recall_at_k, precision_at_k = get_recall_at_k(r_mat_val_edge_index,\n",
        "                                                          r_mat_val_edge_values,\n",
        "                                                          val_pred_ratings,\n",
        "                                                          k = 20\n",
        "                                                         )\n",
        "\n",
        "\n",
        "            val_recall_at_ks.append(round(recall_at_k, 5))\n",
        "            train_losses.append(train_loss.item())\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "            print(f\"[Iteration {iter}/{ITERATIONS}], train_loss: {round(train_loss.item(), 5)}, val_loss: {round(val_loss.item(), 5)},  recall_at_k {round(recall_at_k, 5)}, precision_at_k {round(precision_at_k, 5)}\")\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    if iter % ITERS_PER_LR_DECAY == 0 and iter != 0:\n",
        "        scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 942,
          "referenced_widgets": [
            "9665b6e474ed4cfe8449c4bbfeee60ce",
            "5c0f5258984947e1a8d8b658329ed8d0",
            "4ad7115a014747b4965453fa04ab4e9c",
            "93cf28df996748318ecb8ff7f632aaa9",
            "894c1e55f0d147d589d297f71a53b390",
            "2d0c9ccf8ed74682beb5288477cfd9d6",
            "95187c08eb0345129e0f776deec6640f",
            "312663e29f3d46acb0c61efa2eda56bb",
            "3d02678654384f6480f69a820c6e411c",
            "1dee1e0251b34407b7ed9b5e33f507b8",
            "732df694dee24a89b3013cdf5086abbf"
          ]
        },
        "id": "xAP2jG5nFq2O",
        "outputId": "f3147ee5-c273-479c-ee2a-bf3054269380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9665b6e474ed4cfe8449c4bbfeee60ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iteration 0/10000], train_loss: 16.54942, val_loss: 16.52435,  recall_at_k 0.0, precision_at_k 0.0\n",
            "[Iteration 200/10000], train_loss: 4.75522, val_loss: 4.81642,  recall_at_k 0.0, precision_at_k 0.0\n",
            "[Iteration 400/10000], train_loss: 2.65638, val_loss: 2.69014,  recall_at_k 0.17672, precision_at_k 0.36631\n",
            "[Iteration 600/10000], train_loss: 2.16042, val_loss: 2.17222,  recall_at_k 0.24596, precision_at_k 0.43536\n",
            "[Iteration 800/10000], train_loss: 1.90242, val_loss: 1.90067,  recall_at_k 0.28103, precision_at_k 0.4627\n",
            "[Iteration 1000/10000], train_loss: 1.73559, val_loss: 1.72532,  recall_at_k 0.31374, precision_at_k 0.48645\n",
            "[Iteration 1200/10000], train_loss: 1.61604, val_loss: 1.60048,  recall_at_k 0.34448, precision_at_k 0.50609\n",
            "[Iteration 1400/10000], train_loss: 1.52489, val_loss: 1.50621,  recall_at_k 0.37088, precision_at_k 0.52297\n",
            "[Iteration 1600/10000], train_loss: 1.45243, val_loss: 1.43225,  recall_at_k 0.39255, precision_at_k 0.53558\n",
            "[Iteration 1800/10000], train_loss: 1.39316, val_loss: 1.37253,  recall_at_k 0.41052, precision_at_k 0.54648\n",
            "[Iteration 2000/10000], train_loss: 1.34362, val_loss: 1.32326,  recall_at_k 0.42757, precision_at_k 0.55597\n",
            "[Iteration 2200/10000], train_loss: 1.3015, val_loss: 1.28199,  recall_at_k 0.446, precision_at_k 0.5652\n",
            "[Iteration 2400/10000], train_loss: 1.26523, val_loss: 1.2469,  recall_at_k 0.46225, precision_at_k 0.57271\n",
            "[Iteration 2600/10000], train_loss: 1.23364, val_loss: 1.21673,  recall_at_k 0.47912, precision_at_k 0.57984\n",
            "[Iteration 2800/10000], train_loss: 1.20586, val_loss: 1.1905,  recall_at_k 0.49649, precision_at_k 0.58835\n",
            "[Iteration 3000/10000], train_loss: 1.18123, val_loss: 1.16748,  recall_at_k 0.51208, precision_at_k 0.59486\n",
            "[Iteration 3200/10000], train_loss: 1.15924, val_loss: 1.14708,  recall_at_k 0.52795, precision_at_k 0.60104\n",
            "[Iteration 3400/10000], train_loss: 1.13949, val_loss: 1.12885,  recall_at_k 0.54627, precision_at_k 0.60793\n",
            "[Iteration 3600/10000], train_loss: 1.12165, val_loss: 1.11244,  recall_at_k 0.56585, precision_at_k 0.61513\n",
            "[Iteration 3800/10000], train_loss: 1.10548, val_loss: 1.09756,  recall_at_k 0.58635, precision_at_k 0.62304\n",
            "[Iteration 4000/10000], train_loss: 1.09078, val_loss: 1.08406,  recall_at_k 0.60845, precision_at_k 0.63084\n",
            "[Iteration 4200/10000], train_loss: 1.07742, val_loss: 1.07173,  recall_at_k 0.63382, precision_at_k 0.63951\n",
            "[Iteration 4400/10000], train_loss: 1.06531, val_loss: 1.06051,  recall_at_k 0.66191, precision_at_k 0.64695\n",
            "[Iteration 4600/10000], train_loss: 1.0544, val_loss: 1.05034,  recall_at_k 0.69248, precision_at_k 0.65295\n",
            "[Iteration 4800/10000], train_loss: 1.04472, val_loss: 1.04125,  recall_at_k 0.73704, precision_at_k 0.66098\n",
            "[Iteration 5000/10000], train_loss: 1.03633, val_loss: 1.03332,  recall_at_k 0.78554, precision_at_k 0.66624\n",
            "[Iteration 5200/10000], train_loss: 1.02938, val_loss: 1.02671,  recall_at_k 0.84065, precision_at_k 0.67413\n",
            "[Iteration 5400/10000], train_loss: 1.02412, val_loss: 1.02167,  recall_at_k 0.89777, precision_at_k 0.67767\n",
            "[Iteration 5600/10000], train_loss: 1.02079, val_loss: 1.01852,  recall_at_k 0.91662, precision_at_k 0.67742\n",
            "[Iteration 5800/10000], train_loss: 1.01965, val_loss: 1.01757,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 6000/10000], train_loss: 1.02074, val_loss: 1.01895,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 6200/10000], train_loss: 1.02376, val_loss: 1.02243,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 6400/10000], train_loss: 1.02804, val_loss: 1.02732,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 6600/10000], train_loss: 1.03274, val_loss: 1.03274,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 6800/10000], train_loss: 1.03719, val_loss: 1.03791,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 7000/10000], train_loss: 1.04101, val_loss: 1.04237,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 7200/10000], train_loss: 1.04408, val_loss: 1.04598,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 7400/10000], train_loss: 1.04641, val_loss: 1.04873,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 7600/10000], train_loss: 1.04808, val_loss: 1.05071,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 7800/10000], train_loss: 1.04922, val_loss: 1.05206,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 8000/10000], train_loss: 1.04995, val_loss: 1.05292,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 8200/10000], train_loss: 1.05038, val_loss: 1.05344,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 8400/10000], train_loss: 1.05062, val_loss: 1.05373,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 8600/10000], train_loss: 1.05075, val_loss: 1.05387,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 8800/10000], train_loss: 1.0508, val_loss: 1.05394,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 9000/10000], train_loss: 1.05083, val_loss: 1.05396,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 9200/10000], train_loss: 1.05083, val_loss: 1.05397,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 9400/10000], train_loss: 1.05084, val_loss: 1.05398,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 9600/10000], train_loss: 1.05084, val_loss: 1.05398,  recall_at_k 0.91932, precision_at_k 0.67759\n",
            "[Iteration 9800/10000], train_loss: 1.05084, val_loss: 1.05398,  recall_at_k 0.91932, precision_at_k 0.67759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recommendation Function"
      ],
      "metadata": {
        "id": "eAbJCHzkG8G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_books(user_id, model, lbl_books, num_recommendations, device):\n",
        "    # Assuming 'user_id' is the user for whom you want personalized recommendations\n",
        "    # user_embedding is a learned vector that represents the user representation\n",
        "    user_embedding = model.users_emb(torch.tensor(user_id).to(device))\n",
        "\n",
        "\n",
        "\n",
        "    # Get all item embeddings\n",
        "    all_item_embeddings = model.items_emb(torch.arange(len(lbl_books.classes_)).to(device))\n",
        "\n",
        "    # print(\"all_item_embeddings:  \",all_item_embeddings)\n",
        "\n",
        "    # Compute scores for all items using dot product\n",
        "    scores = torch.mm(all_item_embeddings, user_embedding.view(-1, 1)).squeeze().cpu().detach().numpy()\n",
        "\n",
        "    # print(\"scores: \",scores)\n",
        "\n",
        "    # Sort the items by their scores in descending order\n",
        "    sorted_indices = np.argsort(scores, axis=0)[::-1]\n",
        "\n",
        "    # print(\"sorted_indices: \",sorted_indices)\n",
        "\n",
        "    # # Get the top 'num_recommendations' book indices\n",
        "    top_indices = sorted_indices[:num_recommendations]\n",
        "\n",
        "    # # Map the book indices back to their original labels\n",
        "    recommended_books = lbl_books.classes_[top_indices]\n",
        "\n",
        "    # Create a list of tuples containing book and rating\n",
        "    recommendations_with_ratings = [(book, score) for book, score in zip(recommended_books, scores[top_indices])]\n",
        "    print(\"ratings: \",recommendations_with_ratings)\n",
        "\n",
        "    return recommended_books,recommendations_with_ratings"
      ],
      "metadata": {
        "id": "mdJt9zVMG75D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 87\n",
        "num_recommendations = 5\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "recommended_books,recommendations_with_ratings = recommend_books(user_id, model, lbl_books, num_recommendations, device)\n",
        "\n",
        "print(f\"Top {num_recommendations} recommended books with ratings for user {user_id}:\")\n",
        "for i, (book_id, rating) in enumerate(zip(recommended_books, recommendations_with_ratings)):\n",
        "    book_info = books[books['book_id'] == book_id]\n",
        "    title = book_info['title'].values[0]\n",
        "    # print(f\"{rating[1]:.16f}\")\n",
        "    print(f\"{i+1}. Book ID: {book_id}, Title: {title}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MsNSnPHHBYF",
        "outputId": "7d0c0fb5-f2c8-4c52-b90e-d7031aeee69c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ratings:  [(590, 1.4748727e-07), (69, 1.3807008e-07), (30, 1.0752286e-07), (425, 8.411688e-08), (733, 7.826092e-08)]\n",
            "Top 5 recommended books with ratings for user 87:\n",
            "1. Book ID: 590, Title: Lucene in Action\n",
            "2. Book ID: 69, Title: Ain't No Tomorrow\n",
            "3. Book ID: 30, Title: Earth Magic: A Book of Shadows for Positive Witches\n",
            "4. Book ID: 425, Title: Why Civil Resistance Works: The Strategic Logic of Nonviolent Conflict\n",
            "5. Book ID: 733, Title: As a Driven Leaf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Save the model to a file\n",
        "model_path = 'trained_graph_rec_model.pth'  # Specify the path and filename\n",
        "lbl_books_file_path = 'label_encoder_books.pkl'\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# Optionally, save other relevant information (e.g., LabelEncoder)\n",
        "# For example, to save lbl_books:\n",
        "import joblib\n",
        "joblib.dump(lbl_books, lbl_books_file_path)\n",
        "\n",
        "# Use the files.download function to download the file\n",
        "files.download(model_path)\n",
        "files.download(lbl_books_file_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "myqTR4bl8Cps",
        "outputId": "41dac9c3-bee4-4f1e-85a2-748fc627ddf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fd8bf4a2-4a11-4bb2-a29c-d1df77b3cea0\", \"trained_graph_rec_model.pth\", 2149631)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f53eef9e-83b1-4ef1-81a3-e8370bb8a69e\", \"label_encoder_books.pkl\", 4375)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Function to calculate RMSE and MAE\n",
        "def calculate_rmse_mae(true_ratings, predicted_ratings):\n",
        "    rmse = np.sqrt(mean_squared_error(true_ratings, predicted_ratings))\n",
        "    mae = mean_absolute_error(true_ratings, predicted_ratings)\n",
        "    return rmse, mae\n",
        "\n",
        "# Function to calculate precision and recall\n",
        "def calculate_precision_recall(true_ratings, predicted_ratings, threshold=3.5):\n",
        "    true_positive = sum((true_r >= threshold) and (est >= threshold) for est, true_r in zip(predicted_ratings, true_ratings))\n",
        "    predicted_positive = sum(est >= threshold for est in predicted_ratings)\n",
        "    actual_positive = sum(true_r >= threshold for true_r in true_ratings)\n",
        "\n",
        "    precision = true_positive / predicted_positive if predicted_positive > 0 else 0\n",
        "    recall = true_positive / actual_positive if actual_positive > 0 else 0\n",
        "\n",
        "    return precision, recall\n",
        "\n",
        "# Assuming you have true_ratings and predicted_ratings (e.g., from your validation set)\n",
        "true_ratings = r_mat_val_edge_values.cpu().detach().numpy().flatten()\n",
        "predicted_ratings = val_pred_ratings.cpu().detach().numpy().flatten()\n",
        "\n",
        "# Calculate RMSE and MAE\n",
        "rmse, mae = calculate_rmse_mae(true_ratings, predicted_ratings)\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision, recall = calculate_precision_recall(true_ratings, predicted_ratings, threshold=3.5)\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdFQ1sOx6VSc",
        "outputId": "a4748642-ffe4-472f-e352-803e72f1abfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 1.0266\n",
            "MAE: 0.8247\n",
            "Precision: 0.6682\n",
            "Recall: 1.0000\n"
          ]
        }
      ]
    }
  ]
}